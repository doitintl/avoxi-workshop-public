import {
  __async,
  __forAwait,
  __spreadProps,
  __spreadValues
} from "./chunk-WFI2LP4G.mjs";
import { Message } from "@genkit-ai/ai";
import {
  defineModel,
  GenerationCommonConfigSchema
} from "@genkit-ai/ai/model";
import z from "zod";
const OpenAIConfigSchema = GenerationCommonConfigSchema.extend({
  frequencyPenalty: z.number().min(-2).max(2).optional(),
  logitBias: z.record(z.string(), z.number().min(-100).max(100)).optional(),
  logProbs: z.boolean().optional(),
  presencePenalty: z.number().min(-2).max(2).optional(),
  seed: z.number().int().optional(),
  topLogProbs: z.number().int().min(0).max(20).optional(),
  user: z.string().optional()
});
function toOpenAIRole(role) {
  switch (role) {
    case "user":
      return "user";
    case "model":
      return "assistant";
    case "system":
      return "system";
    case "tool":
      return "tool";
    default:
      throw new Error(`role ${role} doesn't map to an OpenAI role.`);
  }
}
function toOpenAiTool(tool) {
  return {
    type: "function",
    function: {
      name: tool.name,
      parameters: tool.inputSchema
    }
  };
}
function toOpenAiTextAndMedia(part) {
  if (part.text) {
    return {
      type: "text",
      text: part.text
    };
  } else if (part.media) {
    return {
      type: "image_url",
      image_url: {
        url: part.media.url
      }
    };
  }
  throw Error(
    `Unsupported genkit part fields encountered for current message role: ${JSON.stringify(part)}.`
  );
}
function toOpenAiMessages(messages) {
  const openAiMsgs = [];
  for (const message of messages) {
    const msg = new Message(message);
    const role = toOpenAIRole(message.role);
    switch (role) {
      case "user":
        openAiMsgs.push({
          role,
          content: msg.content.map((part) => toOpenAiTextAndMedia(part))
        });
        break;
      case "system":
        openAiMsgs.push({
          role,
          content: msg.text()
        });
        break;
      case "assistant": {
        const toolCalls = msg.content.filter(
          (part) => Boolean(part.toolRequest)
        ).map((part) => {
          var _a;
          return {
            id: (_a = part.toolRequest.ref) != null ? _a : "",
            type: "function",
            function: {
              name: part.toolRequest.name,
              arguments: JSON.stringify(part.toolRequest.input)
            }
          };
        });
        if (toolCalls.length > 0) {
          openAiMsgs.push({
            role,
            tool_calls: toolCalls
          });
        } else {
          openAiMsgs.push({
            role,
            content: msg.text()
          });
        }
        break;
      }
      case "tool": {
        const toolResponseParts = msg.toolResponseParts();
        toolResponseParts.map((part) => {
          var _a;
          openAiMsgs.push({
            role,
            tool_call_id: (_a = part.toolResponse.ref) != null ? _a : "",
            content: typeof part.toolResponse.output === "string" ? part.toolResponse.output : JSON.stringify(part.toolResponse.output)
          });
        });
        break;
      }
    }
  }
  return openAiMsgs;
}
const finishReasonMap = {
  length: "length",
  stop: "stop",
  tool_calls: "stop",
  content_filter: "blocked"
};
function fromOpenAiToolCall(toolCall) {
  if (!toolCall.function) {
    throw Error(
      `Unexpected openAI chunk choice. tool_calls was provided but one or more tool_calls is missing.`
    );
  }
  const f = toolCall.function;
  return {
    toolRequest: {
      name: f.name,
      ref: toolCall.id,
      input: f.arguments ? JSON.parse(f.arguments) : f.arguments
    }
  };
}
function fromOpenAiChoice(choice, jsonMode = false) {
  var _a;
  const toolRequestParts = (_a = choice.message.tool_calls) == null ? void 0 : _a.map(fromOpenAiToolCall);
  return {
    index: choice.index,
    finishReason: finishReasonMap[choice.finish_reason] || "other",
    message: {
      role: "model",
      content: toolRequestParts ? (
        // Note: Not sure why I have to cast here exactly.
        // Otherwise it thinks toolRequest must be 'undefined' if provided
        toolRequestParts
      ) : [
        jsonMode ? { data: JSON.parse(choice.message.content) } : { text: choice.message.content }
      ]
    },
    custom: {}
  };
}
function fromOpenAiChunkChoice(choice, jsonMode = false) {
  var _a;
  const toolRequestParts = (_a = choice.delta.tool_calls) == null ? void 0 : _a.map(fromOpenAiToolCall);
  return {
    index: choice.index,
    finishReason: choice.finish_reason ? finishReasonMap[choice.finish_reason] || "other" : "unknown",
    message: {
      role: "model",
      content: toolRequestParts ? toolRequestParts : [
        jsonMode ? { data: JSON.parse(choice.delta.content) } : { text: choice.delta.content }
      ]
    },
    custom: {}
  };
}
function toRequestBody(model, request) {
  var _a, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p, _q, _r, _s, _t;
  const openAiMessages = toOpenAiMessages(request.messages);
  const mappedModelName = ((_a = request.config) == null ? void 0 : _a.version) || model.version || model.name;
  const body = {
    model: mappedModelName,
    messages: openAiMessages,
    temperature: (_b = request.config) == null ? void 0 : _b.temperature,
    max_tokens: (_c = request.config) == null ? void 0 : _c.maxOutputTokens,
    top_p: (_d = request.config) == null ? void 0 : _d.topP,
    stop: (_e = request.config) == null ? void 0 : _e.stopSequences,
    frequency_penalty: (_f = request.config) == null ? void 0 : _f.frequencyPenalty,
    logit_bias: (_g = request.config) == null ? void 0 : _g.logitBias,
    logprobs: (_h = request.config) == null ? void 0 : _h.logProbs,
    presence_penalty: (_i = request.config) == null ? void 0 : _i.presencePenalty,
    seed: (_j = request.config) == null ? void 0 : _j.seed,
    top_logprobs: (_k = request.config) == null ? void 0 : _k.topLogProbs,
    user: (_l = request.config) == null ? void 0 : _l.user,
    tools: (_m = request.tools) == null ? void 0 : _m.map(toOpenAiTool),
    n: request.candidates
  };
  const response_format = (_n = request.output) == null ? void 0 : _n.format;
  if (response_format) {
    if (response_format === "json" && ((_q = (_p = (_o = model.info) == null ? void 0 : _o.supports) == null ? void 0 : _p.output) == null ? void 0 : _q.includes("json"))) {
      body.response_format = {
        type: "json_object"
      };
    } else if (response_format === "text" && ((_t = (_s = (_r = model.info) == null ? void 0 : _r.supports) == null ? void 0 : _s.output) == null ? void 0 : _t.includes("text"))) {
    } else {
      throw new Error(`${response_format} format is not supported currently`);
    }
  }
  for (const key in body) {
    if (!body[key] || Array.isArray(body[key]) && !body[key].length)
      delete body[key];
  }
  return body;
}
function openaiCompatibleModel(model, clientFactory) {
  const modelId = model.name;
  if (!model)
    throw new Error(`Unsupported model: ${name}`);
  return defineModel(
    __spreadProps(__spreadValues({
      name: modelId
    }, model.info), {
      configSchema: model.configSchema
    }),
    (request, streamingCallback) => __async(this, null, function* () {
      var _a, _b, _c, _d;
      let response;
      const client = yield clientFactory(request);
      const body = toRequestBody(model, request);
      if (streamingCallback) {
        const stream = client.beta.chat.completions.stream(__spreadProps(__spreadValues({}, body), {
          stream: true
        }));
        try {
          for (var iter = __forAwait(stream), more, temp, error; more = !(temp = yield iter.next()).done; more = false) {
            const chunk = temp.value;
            (_a = chunk.choices) == null ? void 0 : _a.forEach((chunk2) => {
              const c = fromOpenAiChunkChoice(chunk2);
              streamingCallback({
                index: c.index,
                content: c.message.content
              });
            });
          }
        } catch (temp) {
          error = [temp];
        } finally {
          try {
            more && (temp = iter.return) && (yield temp.call(iter));
          } finally {
            if (error)
              throw error[0];
          }
        }
        response = yield stream.finalChatCompletion();
      } else {
        response = yield client.chat.completions.create(body);
      }
      return {
        candidates: response.choices.map(
          (c) => {
            var _a2;
            return fromOpenAiChoice(c, ((_a2 = request.output) == null ? void 0 : _a2.format) === "json");
          }
        ),
        usage: {
          inputTokens: (_b = response.usage) == null ? void 0 : _b.prompt_tokens,
          outputTokens: (_c = response.usage) == null ? void 0 : _c.completion_tokens,
          totalTokens: (_d = response.usage) == null ? void 0 : _d.total_tokens
        },
        custom: response
      };
    })
  );
}
export {
  OpenAIConfigSchema,
  fromOpenAiChoice,
  fromOpenAiChunkChoice,
  fromOpenAiToolCall,
  openaiCompatibleModel,
  toOpenAIRole,
  toOpenAiMessages,
  toOpenAiTextAndMedia,
  toRequestBody
};
//# sourceMappingURL=openai_compatibility.mjs.map