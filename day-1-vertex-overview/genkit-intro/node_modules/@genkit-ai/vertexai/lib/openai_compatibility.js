"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __knownSymbol = (name2, symbol) => {
  return (symbol = Symbol[name2]) ? symbol : Symbol.for("Symbol." + name2);
};
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __export = (target, all) => {
  for (var name2 in all)
    __defProp(target, name2, { get: all[name2], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};
var __forAwait = (obj, it, method) => (it = obj[__knownSymbol("asyncIterator")]) ? it.call(obj) : (obj = obj[__knownSymbol("iterator")](), it = {}, method = (key, fn) => (fn = obj[key]) && (it[key] = (arg) => new Promise((yes, no, done) => (arg = fn.call(obj, arg), done = arg.done, Promise.resolve(arg.value).then((value) => yes({ value, done }), no)))), method("next"), method("return"), it);
var openai_compatibility_exports = {};
__export(openai_compatibility_exports, {
  OpenAIConfigSchema: () => OpenAIConfigSchema,
  fromOpenAiChoice: () => fromOpenAiChoice,
  fromOpenAiChunkChoice: () => fromOpenAiChunkChoice,
  fromOpenAiToolCall: () => fromOpenAiToolCall,
  openaiCompatibleModel: () => openaiCompatibleModel,
  toOpenAIRole: () => toOpenAIRole,
  toOpenAiMessages: () => toOpenAiMessages,
  toOpenAiTextAndMedia: () => toOpenAiTextAndMedia,
  toRequestBody: () => toRequestBody
});
module.exports = __toCommonJS(openai_compatibility_exports);
var import_ai = require("@genkit-ai/ai");
var import_model = require("@genkit-ai/ai/model");
var import_zod = __toESM(require("zod"));
const OpenAIConfigSchema = import_model.GenerationCommonConfigSchema.extend({
  frequencyPenalty: import_zod.default.number().min(-2).max(2).optional(),
  logitBias: import_zod.default.record(import_zod.default.string(), import_zod.default.number().min(-100).max(100)).optional(),
  logProbs: import_zod.default.boolean().optional(),
  presencePenalty: import_zod.default.number().min(-2).max(2).optional(),
  seed: import_zod.default.number().int().optional(),
  topLogProbs: import_zod.default.number().int().min(0).max(20).optional(),
  user: import_zod.default.string().optional()
});
function toOpenAIRole(role) {
  switch (role) {
    case "user":
      return "user";
    case "model":
      return "assistant";
    case "system":
      return "system";
    case "tool":
      return "tool";
    default:
      throw new Error(`role ${role} doesn't map to an OpenAI role.`);
  }
}
function toOpenAiTool(tool) {
  return {
    type: "function",
    function: {
      name: tool.name,
      parameters: tool.inputSchema
    }
  };
}
function toOpenAiTextAndMedia(part) {
  if (part.text) {
    return {
      type: "text",
      text: part.text
    };
  } else if (part.media) {
    return {
      type: "image_url",
      image_url: {
        url: part.media.url
      }
    };
  }
  throw Error(
    `Unsupported genkit part fields encountered for current message role: ${JSON.stringify(part)}.`
  );
}
function toOpenAiMessages(messages) {
  const openAiMsgs = [];
  for (const message of messages) {
    const msg = new import_ai.Message(message);
    const role = toOpenAIRole(message.role);
    switch (role) {
      case "user":
        openAiMsgs.push({
          role,
          content: msg.content.map((part) => toOpenAiTextAndMedia(part))
        });
        break;
      case "system":
        openAiMsgs.push({
          role,
          content: msg.text()
        });
        break;
      case "assistant": {
        const toolCalls = msg.content.filter(
          (part) => Boolean(part.toolRequest)
        ).map((part) => {
          var _a;
          return {
            id: (_a = part.toolRequest.ref) != null ? _a : "",
            type: "function",
            function: {
              name: part.toolRequest.name,
              arguments: JSON.stringify(part.toolRequest.input)
            }
          };
        });
        if (toolCalls.length > 0) {
          openAiMsgs.push({
            role,
            tool_calls: toolCalls
          });
        } else {
          openAiMsgs.push({
            role,
            content: msg.text()
          });
        }
        break;
      }
      case "tool": {
        const toolResponseParts = msg.toolResponseParts();
        toolResponseParts.map((part) => {
          var _a;
          openAiMsgs.push({
            role,
            tool_call_id: (_a = part.toolResponse.ref) != null ? _a : "",
            content: typeof part.toolResponse.output === "string" ? part.toolResponse.output : JSON.stringify(part.toolResponse.output)
          });
        });
        break;
      }
    }
  }
  return openAiMsgs;
}
const finishReasonMap = {
  length: "length",
  stop: "stop",
  tool_calls: "stop",
  content_filter: "blocked"
};
function fromOpenAiToolCall(toolCall) {
  if (!toolCall.function) {
    throw Error(
      `Unexpected openAI chunk choice. tool_calls was provided but one or more tool_calls is missing.`
    );
  }
  const f = toolCall.function;
  return {
    toolRequest: {
      name: f.name,
      ref: toolCall.id,
      input: f.arguments ? JSON.parse(f.arguments) : f.arguments
    }
  };
}
function fromOpenAiChoice(choice, jsonMode = false) {
  var _a;
  const toolRequestParts = (_a = choice.message.tool_calls) == null ? void 0 : _a.map(fromOpenAiToolCall);
  return {
    index: choice.index,
    finishReason: finishReasonMap[choice.finish_reason] || "other",
    message: {
      role: "model",
      content: toolRequestParts ? (
        // Note: Not sure why I have to cast here exactly.
        // Otherwise it thinks toolRequest must be 'undefined' if provided
        toolRequestParts
      ) : [
        jsonMode ? { data: JSON.parse(choice.message.content) } : { text: choice.message.content }
      ]
    },
    custom: {}
  };
}
function fromOpenAiChunkChoice(choice, jsonMode = false) {
  var _a;
  const toolRequestParts = (_a = choice.delta.tool_calls) == null ? void 0 : _a.map(fromOpenAiToolCall);
  return {
    index: choice.index,
    finishReason: choice.finish_reason ? finishReasonMap[choice.finish_reason] || "other" : "unknown",
    message: {
      role: "model",
      content: toolRequestParts ? toolRequestParts : [
        jsonMode ? { data: JSON.parse(choice.delta.content) } : { text: choice.delta.content }
      ]
    },
    custom: {}
  };
}
function toRequestBody(model, request) {
  var _a, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p, _q, _r, _s, _t;
  const openAiMessages = toOpenAiMessages(request.messages);
  const mappedModelName = ((_a = request.config) == null ? void 0 : _a.version) || model.version || model.name;
  const body = {
    model: mappedModelName,
    messages: openAiMessages,
    temperature: (_b = request.config) == null ? void 0 : _b.temperature,
    max_tokens: (_c = request.config) == null ? void 0 : _c.maxOutputTokens,
    top_p: (_d = request.config) == null ? void 0 : _d.topP,
    stop: (_e = request.config) == null ? void 0 : _e.stopSequences,
    frequency_penalty: (_f = request.config) == null ? void 0 : _f.frequencyPenalty,
    logit_bias: (_g = request.config) == null ? void 0 : _g.logitBias,
    logprobs: (_h = request.config) == null ? void 0 : _h.logProbs,
    presence_penalty: (_i = request.config) == null ? void 0 : _i.presencePenalty,
    seed: (_j = request.config) == null ? void 0 : _j.seed,
    top_logprobs: (_k = request.config) == null ? void 0 : _k.topLogProbs,
    user: (_l = request.config) == null ? void 0 : _l.user,
    tools: (_m = request.tools) == null ? void 0 : _m.map(toOpenAiTool),
    n: request.candidates
  };
  const response_format = (_n = request.output) == null ? void 0 : _n.format;
  if (response_format) {
    if (response_format === "json" && ((_q = (_p = (_o = model.info) == null ? void 0 : _o.supports) == null ? void 0 : _p.output) == null ? void 0 : _q.includes("json"))) {
      body.response_format = {
        type: "json_object"
      };
    } else if (response_format === "text" && ((_t = (_s = (_r = model.info) == null ? void 0 : _r.supports) == null ? void 0 : _s.output) == null ? void 0 : _t.includes("text"))) {
    } else {
      throw new Error(`${response_format} format is not supported currently`);
    }
  }
  for (const key in body) {
    if (!body[key] || Array.isArray(body[key]) && !body[key].length)
      delete body[key];
  }
  return body;
}
function openaiCompatibleModel(model, clientFactory) {
  const modelId = model.name;
  if (!model)
    throw new Error(`Unsupported model: ${name}`);
  return (0, import_model.defineModel)(
    __spreadProps(__spreadValues({
      name: modelId
    }, model.info), {
      configSchema: model.configSchema
    }),
    (request, streamingCallback) => __async(this, null, function* () {
      var _a, _b, _c, _d;
      let response;
      const client = yield clientFactory(request);
      const body = toRequestBody(model, request);
      if (streamingCallback) {
        const stream = client.beta.chat.completions.stream(__spreadProps(__spreadValues({}, body), {
          stream: true
        }));
        try {
          for (var iter = __forAwait(stream), more, temp, error; more = !(temp = yield iter.next()).done; more = false) {
            const chunk = temp.value;
            (_a = chunk.choices) == null ? void 0 : _a.forEach((chunk2) => {
              const c = fromOpenAiChunkChoice(chunk2);
              streamingCallback({
                index: c.index,
                content: c.message.content
              });
            });
          }
        } catch (temp) {
          error = [temp];
        } finally {
          try {
            more && (temp = iter.return) && (yield temp.call(iter));
          } finally {
            if (error)
              throw error[0];
          }
        }
        response = yield stream.finalChatCompletion();
      } else {
        response = yield client.chat.completions.create(body);
      }
      return {
        candidates: response.choices.map(
          (c) => {
            var _a2;
            return fromOpenAiChoice(c, ((_a2 = request.output) == null ? void 0 : _a2.format) === "json");
          }
        ),
        usage: {
          inputTokens: (_b = response.usage) == null ? void 0 : _b.prompt_tokens,
          outputTokens: (_c = response.usage) == null ? void 0 : _c.completion_tokens,
          totalTokens: (_d = response.usage) == null ? void 0 : _d.total_tokens
        },
        custom: response
      };
    })
  );
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  OpenAIConfigSchema,
  fromOpenAiChoice,
  fromOpenAiChunkChoice,
  fromOpenAiToolCall,
  openaiCompatibleModel,
  toOpenAIRole,
  toOpenAiMessages,
  toOpenAiTextAndMedia,
  toRequestBody
});
//# sourceMappingURL=openai_compatibility.js.map