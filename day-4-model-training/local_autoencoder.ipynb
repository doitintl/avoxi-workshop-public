{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47094ec-e159-4e90-b2b6-09c6016490c1",
   "metadata": {},
   "source": [
    "### Demo for local autoencoder creation & training\n",
    "* steps on how table data can be slowly broken down and later fed to an autoencoder model\n",
    "* steps later provide graphs on potential anomalies detected after recontruction (WIP)\n",
    "* May use but BigQuery autoencoder seems more impactful/robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4de0a-964c-4811-aa9a-710b23a7f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phonenumbers #https://pypi.org/project/phonenumbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1779b77-a927-46a3-927e-7fbe00b51097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries that will be needed for the lab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import os, datetime\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, auc, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065140c-c012-4d7c-8ba0-e32b5e3e7e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'avoxi_workshop_bucket/large_table_cleaned/*.csv'\n",
    "tmp_list = []\n",
    "\n",
    "for csv_file in glob.glob(csv_file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    tmp_series = pd.read_csv(csv_file, delimiter=',', parse_dates=True)\n",
    "    #filter some values\n",
    "    # tmp_series\n",
    "    # break\n",
    "    tmp_list.append(tmp_series)\n",
    "\n",
    "df = pd.concat(tmp_list)\n",
    "#origin_df = df.copy(deep=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a888156-0e0a-4196-bb14-1bb62dd6d5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866bad0-ac97-4d62-b995-85f87ceeaea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c3657-0850-40b4-9983-1a95c8714a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f2253-b51c-40e5-990f-4c2271935640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bc451-79e0-4216-b939-1bc018d188ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the `164_from_caller_id` and `164_to_caller_id` columns to 'Object' datatype\n",
    "df['164_from_caller_id'] = df['164_from_caller_id'].astype(str)\n",
    "df['164_to_caller_id'] = df['164_to_caller_id'].astype(str)\n",
    "\n",
    "# Identify non-numeric columns\n",
    "columns_to_exclude = ['day', 'hour'] # Most likely not needed for normalizing\n",
    "non_numeric_cols = np.concatenate([df.select_dtypes(exclude=[np.number]).columns, columns_to_exclude])\n",
    "numeric_cols = [col for col in df.select_dtypes(include=np.number) if col not in columns_to_exclude]\n",
    "\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "print(f\"Non-numeric columns: {non_numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62befaeb-d4ab-4539-a4da-f4029a4c9329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "# data types\n",
    "# dataset memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93efb2c-bc4e-49e7-a75a-42a95a99d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204ae6e-deec-45bc-8511-f9c3fbc0df89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe and display the result\n",
    "print(\"\\nMissing Values:\\n\")\n",
    "print(df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251e34e-111c-45e3-8ca9-0407bc505a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the statistical summary of the numerical columns in the dataframe and display the result\n",
    "print(\"\\nStatistical Summary:\\n\")\n",
    "print(df.describe().round(5).to_markdown(numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d5b11-8190-464d-939e-727847d9fdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import phonenumbers\n",
    "from phonenumbers import geocoder\n",
    "\n",
    "#https://github.com/azharkhn/libphonenumber-api/blob/master/phonenumber/lib/phonenumber.py\n",
    "def get_E164format(phonenumber):\n",
    "        #phonenumber = self.remove_chars_from_phonenumber(phonenumber)\n",
    "        if(phonenumber[:2] == '00'):\n",
    "            return '+1'+phonenumber\n",
    "        else:\n",
    "            return '+'+phonenumber  \n",
    "\n",
    "def get_country_from_phone(phone_number, country=\"US\"):\n",
    "    #print(get_E164format(phone_number))\n",
    "    formated_num = get_E164format(phone_number)\n",
    "    try:\n",
    "        country = geocoder.country_name_for_number(phonenumbers.parse(formated_num), \"en\")\n",
    "        # Get the country name\n",
    "        return country if country else \"Invalid\" #Empty is invalid\n",
    "    except phonenumbers.phonenumberutil.NumberParseException:\n",
    "        return \"Invalid\"\n",
    "\n",
    "df['from_country'] = df['164_from_caller_id'].astype(str).apply(get_country_from_phone)\n",
    "df['to_country'] = df['164_to_caller_id'].astype(str).apply(get_country_from_phone)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ae03-0b00-4492-9fc3-6f6ba3ebad42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = df.loc[(df['from_country'] != 'Invalid') & (df['to_country'] != 'Invalid')].copy(deep=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb954c75-a224-4855-8898-8dad6b37ad47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(columns=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0549f-683c-423b-a0b6-15acdee86abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6139a1-a504-4f57-81c6-122abd070df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6a3e3-69a6-4901-a785-5b5db98ae8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalies_df = filtered_df.loc[((filtered_df['packet_loss'] >= 0.1) \n",
    "                                & (filtered_df['jitter'] > filtered_df['jitter'].quantile(0.50))) \n",
    "                               & (filtered_df['mean_opinion_score'] < filtered_df['mean_opinion_score'].quantile(0.25))].copy(deep=True)\n",
    "anomalies_df['is_anomaly'] = True\n",
    "anomalies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d97f0d-acea-4986-bfa6-f5a031823e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_anomalies_df = filtered_df.loc[(filtered_df['duration'] > filtered_df['duration'].quantile(0.5)) \n",
    "                                                & (filtered_df['mean_opinion_score'] > 4) \n",
    "                                                & (filtered_df['jitter'] <= 1) \n",
    "                                                & (filtered_df['packet_loss'] <= 0)].copy()\n",
    "no_anomalies_df['is_anomaly'] = False\n",
    "no_anomalies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975459c6-421b-4e24-9796-d5a9032f39c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contamination = (anomalies_df.shape[0]/(anomalies_df.shape[0] + no_anomalies_df.shape[0]))\n",
    "print(f\"Contamination: {int(contamination*100)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd6e27-3e98-43a6-9b2c-64b9586a5dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([anomalies_df, no_anomalies_df])\n",
    "train_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439dbe7-454c-4f61-96bd-1f80d0c55acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[numeric_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols, index=df.index)\n",
    "    result_df = pd.concat([scaled_df, df[non_numeric_cols]], axis=1)\n",
    "    print(\"Processed DataFrame; normalized Dataframe created\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be06140-03af-45b3-870f-45698a4a028e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_df = normalize_dataset(train_test_df)\n",
    "train_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34ffbe-82e8-48e9-bd16-e94bb02cfe77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834784d9-997a-4e2f-84e7-e95ac56ea0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabe336-a784-497d-8566-794e0e5166df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "#X_train, X_test = train_test_split(train_test_df[numeric_cols], test_size=0.1, random_state=42)\n",
    "X_train = train_test_df[numeric_cols]\n",
    "\n",
    "# Input layer\n",
    "input_shape = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa85ea-9462-4fb1-b4db-3b3f0bb773f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9709999-3be7-48e0-a19b-f3546dd7cb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4f4b2-e997-4192-871a-b840eb342894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e913a-d712-4cf8-9c17-75f716b9805a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding layer (hidden layer)\n",
    "# Input layer\n",
    "input_shape = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoded = Dense(input_shape, activation='relu')(input_layer)\n",
    "encoded = Dense(int(input_shape/2), activation='relu')(encoded)\n",
    "\n",
    "# Decoding layer (output layer)\n",
    "decoded = Dense(int(input_shape/2), activation='relu')(encoded)\n",
    "decoded = Dense(input_shape, activation='sigmoid')(decoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "#history = autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size, validation_data=(X_test, X_test))\n",
    "history = autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size)\n",
    "\n",
    "# Print the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a81996-d49c-4d87-bba1-41814dcface9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['loss on train data', 'loss on validation data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404af1c7-a664-4fe3-9c70-8a0cf51318c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "X_origin = filtered_df.sample(n=dataset_size).copy(deep=True)\n",
    "X_test = normalize_dataset(X_origin)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55b44a-84c3-41d5-815c-0269a4ee5cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test_recon = autoencoder.predict(X_test[numeric_cols])\n",
    "\n",
    "# the reconstruction score is the mean of the reconstruction errors (relatively high scores are anomalous)\n",
    "reconstruction_scores = np.mean((X_test[numeric_cols] - x_test_recon)**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156fcdc-196a-4995-80b9-7d3d20f25cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_data = pd.DataFrame({'recon_score':reconstruction_scores})\n",
    "\n",
    "# if our reconstruction scores our normally distributed we can use their statistics\n",
    "anomaly_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9e2a0-9e5b-4e28-bb46-8ee89d8026a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "# calculate mean and standard deviation\n",
    "mse = np.mean(np.square(X_test[numeric_cols] - x_test_recon), axis=1)\n",
    "threshold = np.mean(mse) + 1 * np.std(mse)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "anomalies = np.where(mse > threshold)[0]\n",
    "\n",
    "print(anomalies)\n",
    "#print(mse.iloc[265])\n",
    "\n",
    "mean_mse = np.mean(mse)\n",
    "std_mse = np.std(mse)\n",
    "# define thresholds based on number of standard deviations away from the mean\n",
    "threshold_2std = mean_mse + 2 * std_mse\n",
    "threshold_3std = mean_mse + 3 * std_mse\n",
    "threshold_4std = mean_mse + 4 * std_mse\n",
    "threshold_5std = mean_mse + 5 * std_mse\n",
    "confidences = [100*erf(i/np.sqrt(2)) for i in range(2, 6)]\n",
    "# create a scatter plot of the reconstruction error vs sample index\n",
    "plt.scatter(range(len(mse)), mse)\n",
    "# highlight the anomalies\n",
    "plt.scatter(anomalies, mse.iloc[anomalies], color='red') #anomaly within anomalies (intended behavior?)\n",
    "# add threshold lines\n",
    "plt.axhline(y=threshold_2std, color='green', linestyle='--', label='2 std')\n",
    "plt.text(0.02, threshold_2std + 0.2, f\"2σ ({confidences[0]:.2f}% confidence)\")\n",
    "plt.axhline(y=threshold_3std, color='orange', linestyle='--', label='3 std')\n",
    "plt.text(0.02, threshold_3std + 0.2, f\"3σ ({confidences[1]:.2f}% confidence)\")\n",
    "plt.axhline(y=threshold_4std, color='blue', linestyle='--', label='4 std')\n",
    "plt.text(0.02, threshold_4std + 0.2, f\"4σ ({confidences[2]:.2f}% confidence)\")\n",
    "plt.axhline(y=threshold_5std, color='purple', linestyle='--', label='5 std')\n",
    "plt.text(0.02, threshold_5std + 0.2, f\"5σ ({confidences[3]:.2f}% confidence)\")\n",
    "# add labels and title\n",
    "plt.xlabel('Record Index')\n",
    "plt.ylabel('Reconstruction Error (MSE)')\n",
    "plt.title('Anomaly Detection Results')\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cf51b-0a0b-40fb-b1c9-4e0bd7b62fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_origin.loc[~X_origin.isin(X_origin.iloc[anomalies]).all(axis=1)] #Not anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49f802-9a36-4b34-b5a1-e4ec6ae8e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_origin.loc[X_origin.isin(X_origin.iloc[anomalies]).all(axis=1)] #anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbee0d8-f41e-432b-83a9-f0ebd91de6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_origin['is_anomaly'] = False\n",
    "X_origin['is_anomaly'].iloc[anomalies] = True\n",
    "X_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39773b5-abc7-4da9-a14a-8e21eacd6b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder.save('avoxi_workshop_bucket/saved_model/autoencoder_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c008610-e6da-4aaf-a96d-c33ab82a8013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def insert_dataframe_as_table(project_id, dataset_id, table_id, dataframe):\n",
    "    \"\"\"\n",
    "    Inserts a pandas DataFrame into a BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        dataset_id (str): The BigQuery dataset ID.\n",
    "        table_id (str): The BigQuery table ID.\n",
    "        dataframe (pandas.DataFrame): The DataFrame to insert.\n",
    "    \"\"\"\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Determine schema from DataFrame if not explicitly provided\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True, # Autodetect schema from DataFrame\n",
    "        source_format=bigquery.SourceFormat.PARQUET,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE # Overwrite existing table if it exists. \n",
    "    )\n",
    "\n",
    "    # Load data from DataFrame\n",
    "    job = client.load_table_from_dataframe(\n",
    "        dataframe, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config\n",
    "    )\n",
    "\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Loaded {job.output_rows} rows into {dataset_id}:{table_id}\")\n",
    "\n",
    "# Example Usage (replace with your actual data)\n",
    "project_id = \"gurkomal-playground\"\n",
    "dataset_id = \"avoxi_workshop\"\n",
    "table_id = \"anomaly_labelled_data_v2\"\n",
    "dataframe = ...  # Your Pandas DataFrame\n",
    "\n",
    "insert_dataframe_as_table(project_id, dataset_id, table_id, train_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7dbc0-fbf8-44f5-a84e-fce874b41c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
