{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c3dd42-ff2b-4a63-ac86-548f3b2386a7",
   "metadata": {},
   "source": [
    "### Testing via Local Machine\n",
    "* Use local JupyterNotebook instance to run small pyspark jobs to test preprocessing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ac6b1-01af-4332-a05e-704e297096da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ba1ff-03b0-4716-a0fb-f69b61e58125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cf355-2d8a-44b4-a310-2e707daad61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7ccaf2-5a95-4be2-89da-fc15c17c02e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3164229c-64f5-468f-8c48-8aff40594f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\") # Not used in local but can be useful for naming file suffix. Used for Batch ID in Dataproc Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2baa657d-b46e-47dc-9149-ec0ce756fe25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-10 22:39:26--  https://builds.openlogic.com/downloadJDK/openlogic-openjdk/8u332-b09/openlogic-openjdk-8u332-b09-linux-x64.tar.gz\n",
      "Resolving builds.openlogic.com (builds.openlogic.com)... 13.32.164.34, 13.32.164.69, 13.32.164.53, ...\n",
      "Connecting to builds.openlogic.com (builds.openlogic.com)|13.32.164.34|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105558622 (101M) [application/x-gzip]\n",
      "Saving to: ‘/tmp/java/openlogic-openjdk-8u332-b09-linux-x64.tar.gz’\n",
      "\n",
      "openlogic-openjdk-8 100%[===================>] 100.67M   162MB/s    in 0.6s    \n",
      "\n",
      "2024-09-10 22:39:27 (162 MB/s) - ‘/tmp/java/openlogic-openjdk-8u332-b09-linux-x64.tar.gz’ saved [105558622/105558622]\n",
      "\n",
      "/tmp/java/openlogic-openjdk-8u332-b09-linux-x64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Since the testing suite doesn't support testing on Dataproc clusters,\n",
    "the testing environment is setup to replicate Dataproc via the following steps:\n",
    "\"\"\"\n",
    "JAVA_VER = \"8u332-b09\"\n",
    "JAVA_FOLDER = \"/tmp/java\"\n",
    "FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
    "TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
    "DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
    "PYSPARK_VER = \"3.1.3\"\n",
    "\n",
    "! rm -rf $JAVA_FOLDER\n",
    "! mkdir $JAVA_FOLDER\n",
    "# Download Open JDK 8. Spark requires Java to execute.\n",
    "! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
    "os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
    "! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
    "! echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb55424-3c9a-4567-8b98-906d57040670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing_v2 import run #local file preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ead6451-826a-491f-800d-3b06ed2f84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_gcs_path : str) -> dict:\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    import yaml\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Extract the bucket name and blob (file) name from the config GCS path\n",
    "    bucket_name, blob_name = config_gcs_path.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Download the YAML file from GCS\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    config_data = blob.download_as_text()\n",
    "    config_data = yaml.safe_load(config_data)\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BATCH_ID = \"avoxi-workshop-\" + TIMESTAMP\n",
    "    config_data['batch_id'] = BATCH_ID\n",
    "    return config_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f017997-15bc-4295-897b-9e087dd1220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=get_config(\"gs://avoxi_workshop_bucket/data_pipeline/configuration.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3b1021-fb7c-4873-8088-c52190d23cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS=[\n",
    "    '--input', os.path.expanduser(f\"~/{config['dataproc_args']['input']}\"),\n",
    "    '--output', os.path.expanduser(f\"~/{config['dataproc_args']['output']}\"),\n",
    "    '--anomaly_output', os.path.expanduser(f\"~/{config['dataproc_args']['anomaly_output']}\"),\n",
    "    '--no_anomaly_output', os.path.expanduser(f\"~/{config['dataproc_args']['no_anomaly_output']}\"),\n",
    "    '--anomaly_normalized_output', os.path.expanduser(f\"~/{config['dataproc_args']['anomaly_normalized_output']}\"),\n",
    "    '--no_anomaly_normalized_output', os.path.expanduser(f\"~/{config['dataproc_args']['no_anomaly_normalized_output']}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae614ba-7769-43d2-a3c3-374658e9c1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--input',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/test/',\n",
       " '--output',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/output/v1',\n",
       " '--anomaly_output',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/output/v1/anom/',\n",
       " '--no_anomaly_output',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/output/v1/no_anom/',\n",
       " '--anomaly_normalized_output',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/output/v1/anomaly_normalized/',\n",
       " '--no_anomaly_normalized_output',\n",
       " '/home/jupyter/avoxi_workshop_bucket/data/output/v1/no_anomaly_normalized/']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8457524-7c5e-40ed-b6ad-df49cfca9feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import StringType, NumericType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import phonenumbers\n",
    "from phonenumbers import geocoder\n",
    "\n",
    "#https://github.com/azharkhn/libphonenumber-api/blob/master/phonenumber/lib/phonenumber.py\n",
    "def get_E164format(phonenumber):\n",
    "    if phonenumber[:2] == '00':\n",
    "        return '+1' + phonenumber\n",
    "    else:\n",
    "        return '+' + phonenumber \n",
    "\n",
    "def get_country_from_phone(phone_number):\n",
    "    formated_num = get_E164format(phone_number)\n",
    "    try:\n",
    "        country = geocoder.country_name_for_number(phonenumbers.parse(formated_num), \"en\")\n",
    "        return country if country else \"Invalid\" #Empty is invalid\n",
    "    except phonenumbers.phonenumberutil.NumberParseException:\n",
    "        return \"Invalid\"\n",
    "\n",
    "# Define categorize_columns\n",
    "def categorize_columns(df, list3_columns):\n",
    "    column_types = dict(df.dtypes)\n",
    "    list1 = [col for col, dtype in column_types.items() \n",
    "            if dtype not in ('int', 'double', 'float', 'long')]\n",
    "    list2 = [col for col, dtype in column_types.items() \n",
    "            if dtype in ('int', 'double', 'float', 'long') \n",
    "            and not any(col in l3_col for l3_col in list3_columns)]\n",
    "    return list1, list2\n",
    "\n",
    "def normalize_dataset(df, numeric_cols):\n",
    "    spark = SparkSession.builder.appName(\"NormalizeDataset\").getOrCreate()\n",
    "\n",
    "    # Assemble the numeric columns into a vector\n",
    "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "    assembled_df = assembler.transform(df)\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(assembled_df)\n",
    "    scaled_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "    # Extract the scaled features back into individual columns\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from pyspark.sql.types import DoubleType\n",
    "\n",
    "    def extract_element(vec, index):\n",
    "        return float(vec[index])\n",
    "\n",
    "    extract_udf = udf(extract_element, DoubleType())\n",
    "\n",
    "    for i, col_name in enumerate(numeric_cols):\n",
    "        scaled_df = scaled_df.withColumn(col_name, extract_udf(col(\"scaledFeatures\"), lit(i)))\n",
    "\n",
    "    # Drop the intermediate columns\n",
    "    result_df = scaled_df.drop(\"features\", \"scaledFeatures\")\n",
    "\n",
    "    print(\"Processed DataFrame; normalized DataFrame created\")\n",
    "    return result_df\n",
    "\n",
    "def sparkShape(dataFrame):\n",
    "    return (dataFrame.count(), len(dataFrame.columns))\n",
    "\n",
    "def run(argv=None):\n",
    "    import pyspark\n",
    "    pyspark.sql.dataframe.DataFrame.shape = sparkShape\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input',\n",
    "                        dest='input',\n",
    "                        help='Input file to process.')\n",
    "    parser.add_argument('--output',\n",
    "                        dest='output',\n",
    "                        help='Output file to write results to.')\n",
    "    parser.add_argument('--anomaly_output',\n",
    "                        dest='anomaly_output',\n",
    "                        help='Output file for anomalies to write results to.')\n",
    "    parser.add_argument('--no_anomaly_output',\n",
    "                        dest='no_anomaly_output',\n",
    "                        help='Output file for no anomalies to write results to.')\n",
    "    parser.add_argument('--anomaly_normalized_output',\n",
    "                        dest='anomaly_normalized_output',\n",
    "                        help='Output file for anomalies to write results to.')\n",
    "    parser.add_argument('--no_anomaly_normalized_output',\n",
    "                        dest='no_anomaly_normalized_output',\n",
    "                        help='Output file for no anomalies to write results to.')\n",
    "\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('avoxi_data_parser') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load CSV files into a DataFrame\n",
    "    print(f\"Reading from {known_args.input}\")\n",
    "    df = spark.read.csv(known_args.input, header=True, inferSchema=True)\n",
    " \n",
    "    expected_columns = {\n",
    "        '164_from_caller_id': ['e164_from_caller_id', 'from_number'],\n",
    "        '164_to_caller_id': ['e164_to_caller_id', 'to_number'],\n",
    "        'mean_opinion_score': ['mos'],\n",
    "        'duration': ['duration_seconds']\n",
    "    }\n",
    "\n",
    "    # Loop through the expected columns and rename if necessary\n",
    "    for expected, alternatives in expected_columns.items():\n",
    "        found = False\n",
    "        for actual in alternatives:\n",
    "            if actual in df.columns:\n",
    "                df = df.withColumnRenamed(actual, expected)\n",
    "                found = True\n",
    "                break\n",
    "        if not found and expected not in df.columns:\n",
    "            raise ValueError(f\"None of {expected} or alternatives {alternatives} found in the DataFrame\")\n",
    "        \n",
    "    # Print Schema\n",
    "    df.printSchema()\n",
    "    print(df.shape())\n",
    "\n",
    "    # Remove rows with any null values\n",
    "    df = df.drop('status', 'data_center', 'carrier_id','label','origination','destination')\n",
    "    df = df.dropna()\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Print Schema\n",
    "    print(\"Shape after dropping nulls\")\n",
    "    print(df.shape())\n",
    "    \n",
    "    df = df.withColumn(\"164_from_caller_id\", col(\"164_from_caller_id\").cast(\"string\"))\n",
    "    df = df.withColumn(\"164_to_caller_id\", col(\"164_to_caller_id\").cast(\"string\"))\n",
    "    if 'carrier_id' in columns:\n",
    "        df = df.withColumn(\"carrier_id\", col(\"carrier_id\").cast(\"string\"))\n",
    "    \n",
    "    # Register UDF\n",
    "    get_country_from_phone_udf = udf(get_country_from_phone, StringType())\n",
    "\n",
    "    # Apply the UDF to the DataFrame\n",
    "    df = df.withColumn(\"from_country\", get_country_from_phone_udf(col('164_from_caller_id')))\n",
    "    df = df.withColumn(\"to_country\", get_country_from_phone_udf(col('164_to_caller_id')))\n",
    "\n",
    "    # Filter out invalid phone numbers early\n",
    "    df = df.filter((col('from_country') != 'Invalid') & (col('to_country') != 'Invalid'))\n",
    "\n",
    "    # Print Schema\n",
    "    print(\"Shape after removing invalid countries\")\n",
    "    print(df.shape())\n",
    "    \n",
    "    # Identify non-numeric columns\n",
    "    columns_to_exclude = ['day', 'hour']\n",
    "    non_numeric_cols, numeric_cols = categorize_columns(df, columns_to_exclude)\n",
    "\n",
    "    print(f\"Normalizing columns: {numeric_cols}\")\n",
    "    print(f\"Omitting columns: {non_numeric_cols}\")\n",
    "\n",
    "    \n",
    "    # Step 1: Compute quantiles for reuse\n",
    "    jitter_median = df.approxQuantile('jitter', [0.50], 0.10)[0]\n",
    "    mos_quartile_25 = df.approxQuantile('mean_opinion_score', [0.25], 0.05)[0]\n",
    "    duration_median = df.approxQuantile('duration', [0.50], 0.05)[0]\n",
    "\n",
    "    print(f\"Jitter Median: {jitter_median}\")\n",
    "    print(f\"Mean Opinion Score 25th Quartile: {mos_quartile_25}\")\n",
    "    print(f\"Duration Median: {duration_median}\")\n",
    "    \n",
    "    # Step 2: Create anomalies DataFrame\n",
    "    anomalies_df = df.filter(\n",
    "        (col('packet_loss') > 0.1) &\n",
    "        (col('jitter') > jitter_median) &\n",
    "        (col('mean_opinion_score') < mos_quartile_25)\n",
    "    )\n",
    "\n",
    "    # Step 3: Print Schema and Shape for anomalies_df\n",
    "    anomalies_df.printSchema()\n",
    "    print(f\"Shape of anomalies_df: {anomalies_df.shape()}\")\n",
    "\n",
    "    # Step 4:  Normalized anomalies_df\n",
    "    anomalies_df_normalized = normalize_dataset(anomalies_df, numeric_cols)\n",
    "\n",
    "    # Step 5: Write anomalies DataFrame to GCS\n",
    "    print(f\"Writing anomalies to {known_args.anomaly_output}\")\n",
    "    anomalies_df.write.mode('overwrite').option(\"header\", \"true\").csv(known_args.anomaly_output)\n",
    "    \n",
    "    print(f\"Writing normalized anomalies to {known_args.anomaly_normalized_output}\")\n",
    "    anomalies_df.write.mode('overwrite').option(\"header\", \"true\").csv(known_args.anomaly_normalized_output)\n",
    "\n",
    "    # Step 6: Create no anomalies DataFrame\n",
    "    no_anomalies_df = df.filter(\n",
    "        (col('duration') > duration_median) &\n",
    "        (col('mean_opinion_score') > 4) &\n",
    "        (col('jitter') <= 1) &\n",
    "        (col('packet_loss') <= 0)\n",
    "    )\n",
    "\n",
    "    # Step 7: Print Schema and Shape for no_anomalies_df\n",
    "    no_anomalies_df.printSchema()\n",
    "    print(f\"Shape of no_anomalies_df: {no_anomalies_df.shape()}\")\n",
    "    \n",
    "    # Step 8: Normalized no_anomalies_df\n",
    "    no_anomalies_df_normalized = normalize_dataset(no_anomalies_df, numeric_cols)\n",
    "\n",
    "    # Step 9: Write no anomalies DataFrame to GCS\n",
    "    print(f\"Writing 'normal data' to {known_args.no_anomaly_output}\")\n",
    "    no_anomalies_df.write.mode('overwrite').option(\"header\", \"true\").csv(known_args.no_anomaly_output)\n",
    "    \n",
    "    print(f\"Writing 'normal normalized data' to {known_args.no_anomaly_normalized_output}\")\n",
    "    no_anomalies_df_normalized.write.mode('overwrite').option(\"header\", \"true\").csv(known_args.no_anomaly_normalized_output)    \n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "744559b5-0d1d-445c-b893-aa6accd66438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from /home/jupyter/avoxi_workshop_bucket/data/test/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: string (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- 164_from_caller_id: long (nullable = true)\n",
      " |-- 164_to_caller_id: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- data_center: string (nullable = true)\n",
      " |-- carrier_id: integer (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- mean_opinion_score: double (nullable = true)\n",
      " |-- jitter: integer (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- origination: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n",
      "(304537, 15)\n",
      "Schema after dropping nulls\n",
      "root\n",
      " |-- caller_id: string (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- 164_from_caller_id: long (nullable = true)\n",
      " |-- 164_to_caller_id: long (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- mean_opinion_score: double (nullable = true)\n",
      " |-- jitter: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301363, 9)\n",
      "Schema after removing invalid countries\n",
      "root\n",
      " |-- caller_id: string (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- 164_from_caller_id: string (nullable = true)\n",
      " |-- 164_to_caller_id: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- mean_opinion_score: double (nullable = true)\n",
      " |-- jitter: integer (nullable = true)\n",
      " |-- from_country: string (nullable = true)\n",
      " |-- to_country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173502, 11)\n",
      "Normalizing columns: ['duration', 'packet_loss', 'mean_opinion_score', 'jitter']\n",
      "Omitting columns: ['caller_id', 'organization_id', '164_from_caller_id', '164_to_caller_id', 'start_time', 'from_country', 'to_country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitter Median: 4.0\n",
      "Mean Opinion Score 25th Quartile: 4.2\n",
      "Duration Median: 186.0\n",
      "root\n",
      " |-- caller_id: string (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- 164_from_caller_id: string (nullable = true)\n",
      " |-- 164_to_caller_id: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- mean_opinion_score: double (nullable = true)\n",
      " |-- jitter: integer (nullable = true)\n",
      " |-- from_country: string (nullable = true)\n",
      " |-- to_country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 23:23:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of anomalies_df: (116, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame; normalized DataFrame created\n",
      "Writing anomalies to /home/jupyter/avoxi_workshop_bucket/data/output/v1/anom/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing normalized anomalies to /home/jupyter/avoxi_workshop_bucket/data/output/v1/anomaly_normalized/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: string (nullable = true)\n",
      " |-- organization_id: string (nullable = true)\n",
      " |-- 164_from_caller_id: string (nullable = true)\n",
      " |-- 164_to_caller_id: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- packet_loss: double (nullable = true)\n",
      " |-- mean_opinion_score: double (nullable = true)\n",
      " |-- jitter: integer (nullable = true)\n",
      " |-- from_country: string (nullable = true)\n",
      " |-- to_country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of no_anomalies_df: (6403, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame; normalized DataFrame created\n",
      "Writing 'normal data' to /home/jupyter/avoxi_workshop_bucket/data/output/v1/no_anom/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'normal normalized data' to /home/jupyter/avoxi_workshop_bucket/data/output/v1/no_anomaly_normalized/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0ec44-065b-469b-88ff-5039372e268f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
