{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cca036-0189-4407-88c6-23707faac879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline_v1.py\n",
    "import yaml\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.v1 import bigquery as bq_components\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "import kfp\n",
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import Artifact, Input, Metrics, Output, component, placeholders\n",
    "\n",
    "def get_config(config_gcs_path : str) -> dict:\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Extract the bucket name and blob (file) name from the config GCS path\n",
    "    bucket_name, blob_name = config_gcs_path.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    # Download the YAML file from GCS\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    config_data = blob.download_as_text()\n",
    "    config_data = yaml.safe_load(config_data)\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BATCH_ID = \"avoxi-workshop-\" + TIMESTAMP\n",
    "    config_data['batch_id'] = BATCH_ID\n",
    "    return config_data    \n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-bigquery==3.24.0\"], base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:2.15.0\")\n",
    "def load_bigquery_tables(anomaly_data_path: str, no_anomaly_data_path: str, anomaly_table_name: str, no_anomaly_table_name: str, anomaly_table : Output[artifact_types.BQTable], normal_table: Output[artifact_types.BQTable]):\n",
    "    from google.cloud import bigquery\n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define the external connection for anomaly data\n",
    "    external_config_anomaly = bigquery.ExternalConfig('CSV')\n",
    "    external_config_anomaly.source_uris = [f\"{anomaly_data_path}*.csv\"]\n",
    "    external_config_anomaly.autodetect = True\n",
    "\n",
    "    # Define the external connection for non-anomaly data\n",
    "    external_config_no_anomaly = bigquery.ExternalConfig('CSV')\n",
    "    external_config_no_anomaly.source_uris = [f\"{no_anomaly_data_path}*.csv\"]\n",
    "    external_config_no_anomaly.autodetect = True\n",
    "\n",
    "    # Create or replace the anomaly table\n",
    "    anomaly_table = artifact_types.BQTable.create(name=\"anomaly_table\", project_id=\"gurkomal-playground\", dataset_id=\"avoxi_workshop\",table_id=\"anomaly_table\")\n",
    "    table_anomaly = bigquery.Table(anomaly_table_name)\n",
    "    table_anomaly.external_data_configuration = external_config_anomaly\n",
    "    client.create_table(table_anomaly, exists_ok=True)\n",
    "\n",
    "    # Create or replace the non-anomaly table\n",
    "    normal_table = artifact_types.BQTable.create(name=\"normal_table\", project_id=\"gurkomal-playground\", dataset_id=\"avoxi_workshop\",table_id=\"no_anomaly_table\")\n",
    "    table_no_anomaly = bigquery.Table(no_anomaly_table_name)\n",
    "    table_no_anomaly.external_data_configuration = external_config_no_anomaly\n",
    "    client.create_table(table_no_anomaly, exists_ok=True)\n",
    "\n",
    "@component()\n",
    "def interpret_bqml_evaluation_metrics(\n",
    "    bqml_evaluation_metrics: Input[Artifact], \n",
    "    metrics: Output[Metrics]\n",
    ") -> dict:\n",
    "    import math\n",
    "\n",
    "    metadata = bqml_evaluation_metrics.metadata\n",
    "    for r in metadata[\"rows\"]:\n",
    "\n",
    "        rows = r[\"f\"]\n",
    "        schema = metadata[\"schema\"][\"fields\"]\n",
    "\n",
    "        output = {}\n",
    "        for metric, value in zip(schema, rows):\n",
    "            metric_name = metric[\"name\"]\n",
    "            val = float(value[\"v\"])\n",
    "            output[metric_name] = val\n",
    "            metrics.log_metric(metric_name, val)\n",
    "            if metric_name == \"mean_squared_error\":\n",
    "                rmse = math.sqrt(val)\n",
    "                metrics.log_metric(\"root_mean_squared_error\", rmse)\n",
    "\n",
    "    metrics.log_metric(\"framework\", \"BQML\")\n",
    "    print(output)\n",
    "\n",
    "@component\n",
    "def load_bigquery_model(project_id : str, dataset_id : str, model_id : str, model_registry : str, region : str, model: Output[Artifact]):\n",
    "    project_id = project_id\n",
    "    dataset_id = dataset_id\n",
    "    model_id = model_id\n",
    "    model_registry_id = model_registry\n",
    "    region=region\n",
    "\n",
    "    model.metadata['projectId'] = project_id\n",
    "    model.metadata['datasetId'] = dataset_id\n",
    "    model.metadata['modelId'] = model_id\n",
    "    model.uri = f'projects/{project_id}/datasets/{dataset_id}/models/{model_id}'\n",
    "    model.metadata['resourceName'] = f'projects/{project_id}/locations/{region}/models/{model_registry_id}'\n",
    "\n",
    "@component(base_image=\"python:3.9\")\n",
    "def select_best_model(\n",
    "    metrics_bqml_challenger: Input[Metrics],\n",
    "    metrics_bqml_blessed: Input[Metrics],\n",
    "    thresholds_dict_str: str,\n",
    "    best_metrics: Output[Metrics],\n",
    "    reference_metric_name: str = \"mean_squared_error\",\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"deploy_decision\", str),\n",
    "        (\"best_model\", str),\n",
    "        (\"metric\", float),\n",
    "        (\"metric_name\", str),\n",
    "    ],\n",
    "):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    best_metric = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    metric_bqml_challenger = float(\"inf\")\n",
    "    metric_bqml_blessed = float(\"inf\")\n",
    "\n",
    "    try:\n",
    "        metric_bqml_challenger = metrics_bqml_challenger.metadata[reference_metric_name]\n",
    "        print(f\"Challenger Metric bqml: {metric_bqml_challenger}\")\n",
    "    except:\n",
    "        print(f\"{reference_metric_name} doesn't exist in the BQML dictionary\")\n",
    "\n",
    "    try:\n",
    "        metric_bqml_blessed = metrics_bqml_blessed.metadata[reference_metric_name]\n",
    "        print(f\"Blessed Metric bqml: {metric_bqml_blessed}\")\n",
    "    except:\n",
    "        print(f\"{reference_metric_name} doesn't exist in the BQML dictionary\")\n",
    "\n",
    "    # Change condition if higher is better.\n",
    "    print(f\"Comparing Challenger BQML ({metric_bqml_challenger}) vs Blessed BQML ({metric_bqml_blessed})\")\n",
    "    if metric_bqml_challenger <= metric_bqml_blessed:\n",
    "        best_model = \"blessed bqml\"\n",
    "        best_metric = metric_bqml_blessed\n",
    "        best_metrics.metadata = metrics_bqml_blessed.metadata\n",
    "    else:\n",
    "        best_model = \"challenger bqml\"\n",
    "        best_metric = metric_bqml_challenger\n",
    "        best_metrics.metadata = metrics_bqml_challenger.metadata\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = False\n",
    "\n",
    "    # Change condition if higher is better.\n",
    "    if best_metric < thresholds_dict[reference_metric_name]:\n",
    "        deploy = True\n",
    "\n",
    "    if deploy:\n",
    "        deploy_decision = \"true\"\n",
    "    else:\n",
    "        deploy_decision = \"false\"\n",
    "\n",
    "    print(f\"Which model is best? {best_model}\")\n",
    "    print(f\"What metric is being used? {reference_metric_name}\")\n",
    "    print(f\"What is the best metric? {best_metric}\")\n",
    "    print(f\"What is the threshold to deploy? {thresholds_dict_str}\")\n",
    "    print(f\"Deploy decision: {deploy_decision}\")\n",
    "\n",
    "    Outputs = namedtuple(\n",
    "        \"Outputs\", [\"deploy_decision\", \"best_model\", \"metric\", \"metric_name\"]\n",
    "    )\n",
    "\n",
    "    return Outputs(\n",
    "        deploy_decision=deploy_decision,\n",
    "        best_model=best_model,\n",
    "        metric=best_metric,\n",
    "        metric_name=reference_metric_name,\n",
    "    )\n",
    "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-aiplatform==1.64.0\"])\n",
    "def validate_infrastructure(\n",
    "    endpoint: Input[Artifact],\n",
    ") -> NamedTuple(\n",
    "    \"validate_infrastructure_output\", [(\"instance\", str), (\"prediction\", float)]\n",
    "):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    def treat_uri(uri):\n",
    "        return uri[uri.find(\"projects/\") :]\n",
    "\n",
    "    def request_prediction(endp, instance):\n",
    "        instance = json_format.ParseDict(instance, Value())\n",
    "        instances = [instance]\n",
    "        parameters_dict = {}\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        response = endp.predict(instances=instances, parameters=parameters)\n",
    "        print(\"deployed_model_id:\", response.deployed_model_id)\n",
    "        print(\"predictions: \", response.predictions)\n",
    "        return response.predictions[0]\n",
    "            \n",
    "\n",
    "    endpoint_uri = endpoint.uri\n",
    "    treated_uri = treat_uri(endpoint_uri)\n",
    "\n",
    "    instance = {\n",
    "        \"packet_loss\" : 0.6,\n",
    "        \"duration\" : 100,\n",
    "        \"jitter\" : 1,\n",
    "        \"mean_opinion_score\" : 4.5\n",
    "    }\n",
    "    instance_json = json.dumps(instance)\n",
    "    print(\"Using the following instance: \" + instance_json)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(treated_uri)\n",
    "    prediction = request_prediction(endpoint, instance)\n",
    "    result_tuple = namedtuple(\n",
    "        \"validate_infrastructure_output\", [\"instance\", \"prediction\"]\n",
    "    )\n",
    "    \n",
    "    return result_tuple(instance=str(instance_json), prediction=float(prediction['mean_squared_error']))\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='avox-pipeline')\n",
    "def vertex_ai_pipeline():    \n",
    "    # Preprocess and normalize data using DataprocPySparkBatchOp\n",
    "    from google_cloud_pipeline_components.v1.bigquery import (\n",
    "        BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp,\n",
    "        BigqueryExportModelJobOp, BigqueryPredictModelJobOp,\n",
    "        BigqueryQueryJobOp)\n",
    "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                              ModelDeployOp)\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "    \n",
    "    \n",
    "    config = get_config(config_gcs_path='gs://avoxi_workshop_bucket/data_pipeline/configuration.yaml')\n",
    "\n",
    "    preprocess_job = DataprocPySparkBatchOp(\n",
    "        project=config['project_id'],\n",
    "        location=config['location'],\n",
    "        main_python_file_uri=config['main_python_file'],\n",
    "        #service_account='your-service-account@your-project-id.iam.gserviceaccount.com',\n",
    "        args=[\n",
    "            '--input', config['dataproc_args']['input'],\n",
    "            '--output', config['dataproc_args']['output'],\n",
    "            '--anomaly_output', config['dataproc_args']['anomaly_output'],\n",
    "            '--no_anomaly_output', config['dataproc_args']['no_anomaly_output'],\n",
    "            '--anomaly_normalized_output', config['dataproc_args']['anomaly_normalized_output'],\n",
    "            '--no_anomaly_normalized_output', config['dataproc_args']['no_anomaly_normalized_output']\n",
    "        ],\n",
    "        runtime_config_properties=config['runtime_config_properties'],\n",
    "        batch_id=config['batch_id'],\n",
    "        container_image=config['image_uri']).set_caching_options(config['testing'])\n",
    "    \n",
    "    # Create BigQuery tables\n",
    "    import_data = load_bigquery_tables(\n",
    "        anomaly_data_path=config['dataproc_args']['anomaly_normalized_output'],\n",
    "        no_anomaly_data_path=config['dataproc_args']['no_anomaly_normalized_output'],\n",
    "        anomaly_table_name=config['anomaly_table_name'],\n",
    "        no_anomaly_table_name=config['no_anomaly_table_name']\n",
    "    ).after(preprocess_job)\n",
    "\n",
    "    create_train_op = BigqueryCreateModelJobOp(\n",
    "        project=config['project_id'],\n",
    "        location=config['bq_location'],\n",
    "        query=config['create_model_query'].format(**config)\n",
    "    ).set_caching_options(config['testing']).after(import_data)\n",
    "    \n",
    "    bqml_model = load_bigquery_model(project_id=config['project_id'], dataset_id=config['dataset_id'],\n",
    "                                     model_id=config['model_id'], model_registry=config['model_registry_name'], region=config['location']).after(create_train_op)\n",
    "\n",
    "    # Evaluate model\n",
    "    bqml_evaluate_op = bq_components.BigqueryEvaluateModelJobOp(\n",
    "        project=config['project_id'], \n",
    "        location=config['bq_location'],\n",
    "        model=bqml_model.outputs['model'],\n",
    "        table_name=f\"{config['dataset_id']}.{config['evaluation_table']}\",\n",
    "    ).after(bqml_model)\n",
    "    \n",
    "    bqml_eval_metrics_raw = bqml_evaluate_op.outputs[\"evaluation_metrics\"]\n",
    "    \n",
    "    # Analyzes evaluation BQML metrics using a custom component.\n",
    "    interpret_bqml_evaluation_metrics_op = interpret_bqml_evaluation_metrics(\n",
    "        bqml_evaluation_metrics=bqml_eval_metrics_raw\n",
    "    ).after(bqml_evaluate_op)\n",
    "    \n",
    "    bqml_eval_metrics = interpret_bqml_evaluation_metrics_op.outputs[\"metrics\"]\n",
    "    \n",
    "    # Compare metrics\n",
    "    best_model_task = select_best_model(\n",
    "        metrics_bqml_challenger=bqml_eval_metrics,\n",
    "        metrics_bqml_blessed=bqml_eval_metrics,\n",
    "        thresholds_dict_str=json.dumps(config['model_thresholds']),\n",
    "        reference_metric_name=config['reference_metric_name']\n",
    "    ).after(interpret_bqml_evaluation_metrics_op)    \n",
    "    \n",
    "    # Deploy model\n",
    "    with dsl.If(\n",
    "        ((best_model_task.outputs[\"deploy_decision\"] == \"true\")),\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # Creates a Vertex AI endpoint using a prebuilt component.\n",
    "        endpoint_create_op = EndpointCreateOp(\n",
    "            project=config['project_id'],\n",
    "            location=config['location'],\n",
    "            display_name=config['endpoint_name'],\n",
    "        ).after(best_model_task)\n",
    "        \n",
    "        # Deploys the BQML model (now on Vertex AI) to the recently created endpoint using a prebuilt component.\n",
    "        model_deploy_bqml_op = ModelDeployOp(  # noqa: F841\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=bqml_model.outputs['model'],\n",
    "            deployed_model_display_name=config['deployed_model_display_name'],\n",
    "            dedicated_resources_machine_type=config['dedicated_resources_machine_type'],\n",
    "            dedicated_resources_min_replica_count=config['dedicated_resources_min_replica_count'],\n",
    "            dedicated_resources_max_replica_count=config['dedicated_resources_max_replica_count'],\n",
    "        ).set_caching_options(config['testing']).after(endpoint_create_op)\n",
    "\n",
    "        # Sends an online prediction request to the recently deployed model using a custom component.\n",
    "        validate_infrastructure(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"]\n",
    "        ).set_caching_options(config['testing']).after(model_deploy_bqml_op)\n",
    "            \n",
    "def run_pipeline(config_file: str = \"\", project_id : str = 'gurkomal-playground', location : str = 'us-central1', staging_bucket : str = \"gs://avoxi_workshop_bucket/staging_pipeline\"):\n",
    "    \n",
    "    experiment_name=\"avoxi-experiment-2\"\n",
    "    experiment_description=\"Avoxi Workshop\"\n",
    "    \n",
    "    aiplatform.init(\n",
    "        project=project_id, \n",
    "        location=location,\n",
    "        experiment=experiment_name,\n",
    "        experiment_description=experiment_description,\n",
    "        experiment_tensorboard=False,\n",
    "    )\n",
    "    \n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=vertex_ai_pipeline,\n",
    "        package_path=\"vertex-ai-pipeline.json\"\n",
    "    )\n",
    "    \n",
    "    aiplatform.PipelineJob(\n",
    "        display_name='vertex-ai-pipeline',\n",
    "        template_path=\"vertex-ai-pipeline.json\",\n",
    "        pipeline_root=f\"{staging_bucket}/pipeline_root/vertex-ai-pipeline\",\n",
    "        enable_caching=False\n",
    "    ).submit(\n",
    "        experiment=experiment_name\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    # if len(sys.argv) != 2:\n",
    "    #     print(\"Usage: python pipeline.py <config_file>\")\n",
    "    #     sys.exit(1)   \n",
    "    # else:\n",
    "    #     config_file = sys.argv[1]\n",
    "        #config = set_config(config_file)\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78e702b-6458-48d6-8ec1-b140475b43d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.8' to 'python:3.9' on Oct 1, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.9.\n",
      "  return component_factory.create_component_from_func(\n",
      "<IPython.core.display.HTML object>\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/506365831141/locations/us-central1/pipelineJobs/avox-pipeline-20240912141421\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/506365831141/locations/us-central1/pipelineJobs/avox-pipeline-20240912141421')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/avox-pipeline-20240912141421?project=506365831141\n",
      "Associating projects/506365831141/locations/us-central1/pipelineJobs/avox-pipeline-20240912141421 to Experiment: avoxi-experiment-2\n"
     ]
    }
   ],
   "source": [
    "!python \"pipeline_v1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986aadac-cce3-4d57-b495-9becdec386bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
